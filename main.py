import openai
import subprocess
import os
import random
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# OpenAI APIã‚­ãƒ¼ã‚’å–å¾—
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ç’°å¢ƒå¤‰æ•° 'OPENAI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
client = openai.OpenAI(api_key=api_key)

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆ
output_file = "C:\\Users\\koshi\\Work\\PromptMotion\\output.mp3"
temp_dir = "C:\\Users\\koshi\\Work\\PromptMotion\\temp"
dancers_file = "C:\\Users\\koshi\\Work\\PromptMotion\\dancers.txt"

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
if not os.path.exists(temp_dir):
    os.makedirs(temp_dir)

def load_dancers():
    """ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ€ãƒ³ã‚µãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ """
    if not os.path.exists(dancers_file):
        raise FileNotFoundError(f"ãƒ€ãƒ³ã‚µãƒ¼ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dancers_file}")

    with open(dancers_file, "r", encoding="utf-8") as file:
        dancers = [line.strip() for line in file.readlines() if line.strip()]
    
    if not dancers:
        raise ValueError("ãƒ€ãƒ³ã‚µãƒ¼ã®ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚")

    return dancers

def generate_instructions():
    """ 1å›ã®APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã§10å€‹ã®ãƒ€ãƒ³ã‚¹ã®æŒ‡ç¤ºã‚’ç”Ÿæˆ """
    prompt = """
    ã‚ãªãŸã¯ãƒ€ãƒ³ã‚¹ã®æŒ‡å°è€…ã§ã™ã€‚
    ä»¥ä¸‹ã®æ¡ä»¶ã«å¾“ã„ã€10å€‹ã®ç•°ãªã‚‹ãƒ€ãƒ³ã‚¹å‹•ä½œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

    æ¡ä»¶:
    - å„å‹•ä½œã¯çŸ­ãã€ç°¡æ½”ã«
    - å…·ä½“çš„ã§æ˜ç¢ºãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    - ä¾‹: ã€Œå³æ‰‹ã‚’ä¸Šã’ã‚‹ã€ã€Œå·¦è¶³ã‚’ä¸€æ­©å‰ã«å‡ºã™ã€

    10å€‹ã®å‹•ä½œã‚’æ”¹è¡Œã§åŒºåˆ‡ã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": prompt}]
    )

    # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆåŒ–
    instructions = response.choices[0].message.content.strip().split("\n")
    
    if len(instructions) < 10:
        raise ValueError("OpenAI APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒä¸å®Œå…¨ã§ã™ã€‚")

    return instructions

def assign_instructions_to_dancers(instructions, dancers):
    """ ãƒ€ãƒ³ã‚µãƒ¼ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªå‹•ä½œã‚’å‰²ã‚Šå½“ã¦ã€æ•°å­—ã‚’å‰Šé™¤ã™ã‚‹ """
    assignments = []
    for instruction in instructions:
        dancer = random.choice(dancers)  # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ€ãƒ³ã‚µãƒ¼ã‚’é¸ã¶
        # å…ˆé ­ã®æ•°å­—ï¼ˆ1. ãªã©ï¼‰ã‚’å‰Šé™¤
        cleaned_instruction = " ".join(instruction.split()[1:]) if instruction[0].isdigit() else instruction
        assignments.append(f"{dancer} {cleaned_instruction}")

    print("\nğŸ“¢ **ãƒ€ãƒ³ã‚µãƒ¼ã”ã¨ã®æŒ‡ç¤º:**")
    for assignment in assignments:
        print(assignment)

    return assignments


def create_silent_audio(duration, output_file):
    """ æŒ‡å®šã—ãŸç§’æ•°ã®ç„¡éŸ³mp3ã‚’ä½œæˆ """
    command = f'ffmpeg -y -f lavfi -i anullsrc=r=24000:cl=mono -t {duration} -q:a 9 -acodec libmp3lame "{output_file}"'
    subprocess.run(command, shell=True)

def speak_text_with_silence(texts):
    """ æŒ‡ç¤ºã”ã¨ã«ãƒ€ãƒ³ã‚µãƒ¼åã®å¾Œã«1.5ç§’ã®ç„¡éŸ³ã‚’æŒ¿å…¥ã—ã€æŒ‡ç¤ºé–“ã®ç„¡éŸ³ã‚’2ã€œ10ç§’ãƒ©ãƒ³ãƒ€ãƒ ã«æŒ¿å…¥ã—ãªãŒã‚‰ mp3 ã‚’ä½œæˆ """
    temp_files = []

    for i, text in enumerate(texts):
        parts = text.split(" ", 1)  # æœ€åˆã®ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
        if len(parts) < 2:
            continue  # å¿µã®ãŸã‚ã€æŒ‡ç¤ºãŒåˆ†å‰²ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

        dancer_name = parts[0]
        instruction = parts[1]

        temp_dancer_file = f"{temp_dir}\\dancer_{i}.mp3"
        temp_silence_file = f"{temp_dir}\\silence_{i}.mp3"
        temp_instruction_file = f"{temp_dir}\\instruction_{i}.mp3"
        temp_pause_file = f"{temp_dir}\\pause_{i}.mp3"  # æŒ‡ç¤ºé–“ã®ãƒ©ãƒ³ãƒ€ãƒ ãªç„¡éŸ³

        temp_files.append(temp_dancer_file)
        temp_files.append(temp_silence_file)
        temp_files.append(temp_instruction_file)
        temp_files.append(temp_pause_file)

        # ãƒ€ãƒ³ã‚µãƒ¼åã®éŸ³å£°ç”Ÿæˆ
        command_dancer = f'edge-tts --text "{dancer_name}" --voice ja-JP-NanamiNeural --rate=+5% --volume=+0% --write-media "{temp_dancer_file}"'
        subprocess.run(command_dancer, shell=True)

        # 1.5ç§’ã®ç„¡éŸ³ã‚’ç”Ÿæˆï¼ˆãƒ€ãƒ³ã‚µãƒ¼åã¨æŒ‡ç¤ºã®é–“ï¼‰
        create_silent_audio(1.5, temp_silence_file)

        # æŒ‡ç¤ºæ–‡ã®éŸ³å£°ç”Ÿæˆ
        command_instruction = f'edge-tts --text "{instruction}" --voice ja-JP-NanamiNeural --rate=+5% --volume=+0% --write-media "{temp_instruction_file}"'
        subprocess.run(command_instruction, shell=True)

        # 2ã€œ10ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ ãªç„¡éŸ³ã‚’ç”Ÿæˆï¼ˆæŒ‡ç¤ºã¨æŒ‡ç¤ºã®é–“ï¼‰
        silence_duration = random.randint(2, 10)
        create_silent_audio(silence_duration, temp_pause_file)

    # ã™ã¹ã¦ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
    concat_list = "|".join(temp_files)
    merge_command = f'ffmpeg -y -i "concat:{concat_list}" -acodec copy "{output_file}"'
    subprocess.run(merge_command, shell=True)

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    for file in temp_files:
        os.remove(file)

    # Windowsã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å†ç”Ÿ
    subprocess.Popen(["start", "", output_file], shell=True)

if __name__ == "__main__":
    # 1. ãƒ€ãƒ³ã‚µãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€
    dancers = load_dancers()

    # 2. AIãŒãƒ€ãƒ³ã‚¹ã®æŒ‡ç¤ºã‚’10å€‹ç”Ÿæˆ
    instructions = generate_instructions()

    # 3. ãƒ€ãƒ³ã‚µãƒ¼ã«ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦
    assigned_instructions = assign_instructions_to_dancers(instructions, dancers)

    # 4. éŸ³å£°ã‚’ç”Ÿæˆã—ã€ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã§æŒ‡ç¤ºã‚’èª­ã¿ä¸Šã’ã‚‹
    print("\nğŸ¤ ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã§æŒ‡ç¤ºã‚’èª­ã¿ä¸Šã’ã¾ã™...")
    speak_text_with_silence(assigned_instructions)
